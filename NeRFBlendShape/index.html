<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reconstructing Personalized Semantic Facial NeRF Models From Monocular Video">
  <meta name="keywords" content="Blendshape, Neural Radiance Field, Facial
  Retargeting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NeRFBlendShape</title>

  <!-- Bootstrap -->
  <link href="static/css/bootstrap-4.4.1.css" rel="stylesheet">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 2">Reconstructing Personalized Semantic Facial NeRF Models From
            Monocular Video</h2>
            <!-- <h2 class="title is-4 publication-title" style="margin-top: 0; margin-bottom: 0">SIGGRAPH Asia 2022 (Journal Track)</h2> -->
          <h2 class="title is-4 publication-title" style="color:#6e6e6e;margin-top: 2; margin-bottom: 2">SIGGRAPH Asia 2022 (Journal Track)</h2>
          <!-- <h2 class="title is-2 publication-title" style="margin-top: 0"></h2> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://xuanghahahaha.github.io/">Xuan Gao</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">Chenglai Zhong</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">Jun Xiang</a><sup>1</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"> <a href="https://hy1995.top/">Yang Hong</a>
              <sup>1</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"> <a href="https://yudongguo.github.io/">Yudong Guo</a>
              <sup>2</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Image Derivative Inc</span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">(This work was done when Xuan Gao, Chenglai Zhong and Jun Xiang were intern at Image Derivative Inc.)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links" style="margin-top: 0; margin-bottom: 2">
              <!-- PDF Link. -->
              <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2210.06108"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!--
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/USTC3DV/NeRFBlendShape-code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/teaser.png"
                type="video/mp4">
      </video>
      -->
      <img src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/teaser.jpg" class="center">
      <h2 class="subtitle has-text-centered" style="margin-top: 15px">
        TL;DR: A semantic parametric model for human head defined with neural radiance field. Multi-level voxel field is adopted as basis with corresponding expression coefficients, which enables strong representation ability on the aspect of rendering and fast training.
      </h2>
    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reconstructions</h2>

        <div class="embed-responsive embed-responsive-16by9">

          <iframe style="clip-path: inset(1px 1px)" src="https://sketchfab.com/playlists/embed?collection=abee3cc1a7a7436c804f2bd3aadc2acd" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true" width="100%" height="100%" frameborder="0"></iframe>
        </div>
        

      </div>
    </div>

  </div>
</section>

-->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a novel semantic model for human head defined with neural radiance field. The 3D-consistent head model consists of a set of disentangled and interpretable bases, and can be driven by low-dimensional expression coefficients. Thanks to the powerful representation ability of neural radiance field, the constructed model can represent complex facial attributes including hair and wearings, which can not be represented by traditional mesh blendshape. To construct the personalized semantic facial model, we propose to define the bases as several multi-level voxel fields. With a short monocular RGB video as input, our method can construct the subject's semantic facial NeRF model in only ten to twenty minutes and can render a photo-realistic human head image in tens of milliseconds with a given expression coefficient and view direction. With this novel representation, we apply it to many tasks like facial retargeting and expression editing. Experimental results demonstrate its strong representation ability and training/inference speed.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method</h2>

        <img src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/pipeline.jpg" class="center">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            We track the RGB sequence and get expression coefficients, poses, and intrinsics. Then we use the tracked expression coefficients to combine multiple multi-level hash tables to get a hash table corresponding to a specific expression. Then the sampled point is queried in the hash table to get voxel features, we use an MLP to interpret the voxel features as RGB and density. We fix the expression coefficients and optimize the hash tables and MLP to get our head model.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/method.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Facial Reenactment</h2>

        <div class="content has-text-justified">
          <p>
            Our model can be easily used for facial reenactment. We track the source subjects' video and get poses, camera intrinsics, and expression coefficients. The poses and intrinsics are utilized to cast rays, and we further use expression coefficients to combine targets' bases. Then the sampled point is queried in the final hash table and interpreted by MLP to predict RGB and density. 
          <!-- </p>
          <p> -->
            Compared with other methods, our model demonstrates more personalized facial details and synthesizes a more reasonable human head conditioned on expression coefficients both in self-reenactment and cross-identity reenactment.
          </p>
        </div>
        <h3 class="title is-4">Self-reenactment</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/self.mp4"
                    type="video/mp4">
          </video>
        </div>
        <h3 class="title is-4">Cross-identity Reenactment</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/cross.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Comparisons. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Training
        </h2>

        <div class="content has-text-justified">
          <p>
            Our model could converge to a reasonable result in 20 minutes. The implicit blendshape architecture plays an important role in the learning process.
            Compared with a baseline model concatenating multi-level hash features with expression coefficients, our model could learn a dynamic head scene much faster.
            
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/training.mp4"
                    type="video/mp4">
          </video>
        </div>


      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Ablation Studies. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Novel View Synthesis</h2>

        <div class="content has-text-justified">
          <p>
            Thanks to the 3D consistency of NeRF, our model could also disentangle the camera parameters. Thus we can freely adjust the camera parameters of our model to generate novel view synthesis results. We first use a set of expression coefficients from the testset to combine bases to get the corresponding radiance field. Then the rendered images with different rendering views are generated by the volume rendering.
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="https://raw.githubusercontent.com/XuanGhahahaha/project_page_assets/master/NeRFBlendShape/resources/nvs.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <div class="content has-text-justified">
      <p>
        If you find our paper useful for your work please cite:
      </p>
    </div>
    <pre><code>
      @article{Gao2022nerfblendshape,
         author = {Xuan Gao and Chenglai Zhong and Jun Xiang and Yang Hong and Yudong Guo and Juyong Zhang}, 
         title = {Reconstructing Personalized Semantic Facial NeRF Models From Monocular Video}, 
         journal = {ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia)}, 
         volume = {41}, 
         number = {6}, 
         year = {2022}, 
         doi = {10.1145/3550454.3555501} }
    </code></pre>
  </div>
</section>



<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    This research was supported by the National Natural Science Foundation of China (No.62122071, No.62272433), and the Fundamental Research Funds for the Central Universities (No. WK3470000021).
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

</body>
</html>
